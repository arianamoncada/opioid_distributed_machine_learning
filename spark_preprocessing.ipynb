{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time \n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drugs = pd.read_csv('arcos-ca-statewide-itemized.tsv',nrows = 1000000, sep = '\\t')\n",
    "# drugs = pd.read_csv('arcos-ca-statewide-itemized.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"org.apache.hadoop:hadoop-aws:2.7.4\" pyspark-shell'\n",
    "!echo $JAVA_HOME\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIA4A3YLKCTSGOUA35B\")\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key', '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')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drugs = pd.read_csv('../arcos-ca-statewide-sample.tsv', sep = '\\t')\n",
    "# drugs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on sample of Drug Dataset to make a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.sql.functions import concat, col, lit, substring\n",
    "\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "\n",
    "## S3 Bucket\n",
    "drug_rdd = sc.textFile('../arcos-ca-statewide-sample.tsv').map(lambda x: x.split('\\t'))\n",
    "#drug_rdd = sc.textFile('s3://data-systems-opioid/arcos-ca-statewide-itemized.tsv',24).map(lambda x: x.split('\\t'))\n",
    "\n",
    "\n",
    "def FloatSafe(value): # In case there are non-float type to be converted.\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def IntegerSafe(value): # In case there are non-integer type to be converted.\n",
    "    try:\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# To reduce size, I remove the first 15 col's which are all unusable Identifiers\n",
    "drug_rdd = drug_rdd.map(lambda x: x[16:25] + x[29:] + [x[11]]).persist()\n",
    "\n",
    "# Columns I removed:\n",
    "# -------------------------\n",
    "# 0'REPORTER_DEA_NO',\n",
    "#  'REPORTER_BUS_ACT',\n",
    "#  'REPORTER_NAME',\n",
    "#  'REPORTER_ADDL_CO_INFO',\n",
    "#  'REPORTER_ADDRESS1',\n",
    "#  'REPORTER_ADDRESS2',\n",
    "#  'REPORTER_CITY',\n",
    "#  'REPORTER_STATE',\n",
    "#  'REPORTER_ZIP',\n",
    "#  'REPORTER_COUNTY'\n",
    "#10'BUYER_DEA_NO',\n",
    "#12 'BUYER_NAME',\n",
    "#  'BUYER_ADDL_CO_INFO',\n",
    "#  'BUYER_ADDRESS1',\n",
    "#15'BUYER_ADDRESS2'\n",
    "\n",
    "# 25 UNIT (0.001% of rows have values)\n",
    "# 26 Action Indicator\n",
    "# 27 ORDER_FORM_NO\n",
    "# 28 CORRECTION_NO\n",
    "\n",
    "# Takes header row and makes column names\n",
    "col_names = drug_rdd.first()\n",
    "\n",
    "# Removes header col\n",
    "drug_rdd = drug_rdd.filter(lambda x: x != col_names)\n",
    "\n",
    "# Fixes variable type\n",
    "drug_rdd = drug_rdd.map(lambda x: [x[0], x[1], x[2], x[3], x[4], IntegerSafe(x[5]), x[6], x[7], FloatSafe(x[8]), FloatSafe(x[9]), IntegerSafe(x[10]), FloatSafe(x[11]), FloatSafe(x[12]), IntegerSafe(x[13]),x[14], x[15], x[16], FloatSafe(x[17]), x[18], x[19], x[20], FloatSafe(x[21]), x[22]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BUYER_CITY', 'BUYER_STATE', 'BUYER_ZIP', 'BUYER_COUNTY', 'TRANSACTION_CODE', 'DRUG_CODE', 'NDC_NO', 'DRUG_NAME', 'QUANTITY', 'STRENGTH', 'TRANSACTION_DATE', 'CALC_BASE_WT_IN_GM', 'DOSAGE_UNIT', 'TRANSACTION_ID', 'Product_Name', 'Ingredient_Name', 'Measure', 'MME_Conversion_Factor', 'Combined_Labeler_Name', 'Revised_Company_Name', 'Reporter_family', 'dos_str', 'BUYER_BUS_ACT']\n"
     ]
    }
   ],
   "source": [
    "print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell is just for Type testing\n",
    "# row_test = drug_rdd.takeSample(1, 1)[0]\n",
    "# for i in range(len(row_test)):\n",
    "#     print(i, col_names[i], row_test[i], type(row_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+\n",
      "|BUYER_ZIP|Year|  ZIP-YEAR|\n",
      "+---------+----+----------+\n",
      "|    93003|2007|93003-2007|\n",
      "|    92649|2006|92649-2006|\n",
      "|    92653|2006|92653-2006|\n",
      "|    92113|2006|92113-2006|\n",
      "|    92113|2006|92113-2006|\n",
      "|    91301|2007|91301-2007|\n",
      "|    92584|2007|92584-2007|\n",
      "|    93402|2012|93402-2012|\n",
      "|    93065|2012|93065-2012|\n",
      "|    95536|2011|95536-2011|\n",
      "+---------+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To DataFrame\n",
    "drug_df = drug_rdd.toDF(col_names)\n",
    "\n",
    "# Set up for the ZIP-YEAR join\n",
    "drug_df = drug_df.withColumn('Year', substring('TRANSACTION_DATE', -4,4))\n",
    "drug_df = drug_df.withColumn('ZIP-YEAR', concat(col(\"BUYER_ZIP\"), lit(\"-\"), col(\"Year\")))\n",
    "drug_df.select(\"BUYER_ZIP\",\"Year\",'ZIP-YEAR').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug Dataset feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Becuase we aggregate everything, we may not have to worry about removing nulls\n",
    "\n",
    "#converting strings to numeric values\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def indexStringColumns(df, cols):\n",
    "    #variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        #For each given colum, fits StringIndexerModel, it knows what the unique values are\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf\n",
    "\n",
    "drug_df = indexStringColumns(drug_df, [\"BUYER_COUNTY\", \"Revised_Company_Name\",\"DRUG_NAME\", \"BUYER_BUS_ACT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "drug_df = oneHotEncodeColumns(drug_df, [\"DRUG_NAME\", \"BUYER_BUS_ACT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------------+-------------+\n",
      "|BUYER_COUNTY|Revised_Company_Name|    DRUG_NAME|BUYER_BUS_ACT|\n",
      "+------------+--------------------+-------------+-------------+\n",
      "|        11.0|                56.0|(2,[0],[1.0])|(6,[2],[1.0])|\n",
      "|         1.0|                 1.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|         1.0|                 6.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|         2.0|                 2.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|         2.0|                 0.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|         0.0|                 0.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|         3.0|                 3.0|(2,[1],[1.0])|(6,[1],[1.0])|\n",
      "|        22.0|                 2.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|        11.0|                 2.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|        28.0|                 0.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|        28.0|                 0.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|        28.0|                 0.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|        28.0|                 0.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|        28.0|                 2.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|         4.0|                 4.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|        32.0|                 1.0|(2,[0],[1.0])|(6,[1],[1.0])|\n",
      "|        11.0|                 0.0|(2,[1],[1.0])|(6,[0],[1.0])|\n",
      "|        11.0|                 2.0|(2,[1],[1.0])|(6,[0],[1.0])|\n",
      "|        11.0|                 1.0|(2,[1],[1.0])|(6,[0],[1.0])|\n",
      "|        11.0|                 0.0|(2,[1],[1.0])|(6,[0],[1.0])|\n",
      "+------------+--------------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drug_df.select(\"BUYER_COUNTY\", \"Revised_Company_Name\",\"DRUG_NAME\", \"BUYER_BUS_ACT\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(6, {2: 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_df.select(\"BUYER_BUS_ACT\").first()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suicide DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_rdd = sc.textFile('s3://data-systems-opioid/CA_suicides.csv',24).map(lambda x: x.split(','))\n",
    "\n",
    "# Takes header row and makes column names\n",
    "col_names = death_rdd.first()\n",
    "\n",
    "# Removes header col\n",
    "death_rdd = death_rdd.filter(lambda x: x != col_names)\n",
    "\n",
    "# Fix RDD\n",
    "death_rdd = death_rdd.map(lambda x: [x[0], x[1], x[2], IntegerSafe(x[3]), IntegerSafe(x[4])])\n",
    "\n",
    "# To SQL DataFrame\n",
    "death_df = death_rdd.toDF(col_names)\n",
    "death_df = death_df.withColumn('SUI_per_thousand', death_df['Count']/death_df['Population_2018'] * 1000)\n",
    "\n",
    "# Set Up for the ZIP-YEAR merg\n",
    "death_df = death_df.withColumn('ZIP-YEAR', concat(col(\"ZIP Code\"), lit(\"-\"), col(\"Year\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining drug_df and death_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+-------------+-------------+-----------------------+------------+------------------+\n",
      "|  ZIP-YEAR|min(BUYER_COUNTY)|count(BUYER_CITY)|sum(STRENGTH)|sum(QUANTITY)|sum(CALC_BASE_WT_IN_GM)|sum(dos_str)|  avg(DOSAGE_UNIT)|\n",
      "+----------+-----------------+-----------------+-------------+-------------+-----------------------+------------+------------------+\n",
      "|93313-2007|              9.0|              218|          0.0|        529.0|             559.139049|     2042.85|510.55045871559633|\n",
      "|91790-2008|              0.0|              193|          0.0|        841.0|      908.5731599999999|      2452.5|487.04663212435236|\n",
      "|92008-2007|              2.0|              126|          0.0|        387.0|              383.30595|      1370.0|  513.968253968254|\n",
      "|92833-2007|              1.0|              132|          0.0|        210.0|               325.9797|      1575.0|377.27272727272725|\n",
      "|91910-2012|              2.0|              416|      28000.0|       1034.0|         1088.251927725|    4304.671|427.90865384615387|\n",
      "+----------+-----------------+-----------------+-------------+-------------+-----------------------+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change this join based on what flags we add\n",
    "drug_agg_df = drug_df.groupBy('ZIP-YEAR').agg(\n",
    "    min('BUYER_COUNTY'), # Should only be one per ZIP\n",
    "    count('BUYER_CITY'),\n",
    "    sum('STRENGTH'),\n",
    "    sum('QUANTITY'),\n",
    "    sum('CALC_BASE_WT_IN_GM'),\n",
    "    sum('dos_str'),\n",
    "    avg('DOSAGE_UNIT'),\n",
    "    # sum('DRUG_NAME') NOT ABLE TO aggregate due to index issue!\n",
    ")\n",
    "drug_agg_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+-------------+-------------+-----------------------+------------------+------------------+-----+--------------------+\n",
      "|  ZIP-YEAR|min(BUYER_COUNTY)|count(BUYER_CITY)|sum(STRENGTH)|sum(QUANTITY)|sum(CALC_BASE_WT_IN_GM)|      sum(dos_str)|  avg(DOSAGE_UNIT)|Count|    SUI_per_thousand|\n",
      "+----------+-----------------+-----------------+-------------+-------------+-----------------------+------------------+------------------+-----+--------------------+\n",
      "|90026-2007|              0.0|               88|          0.0|        142.0|     271.17572900000005|           1172.85| 341.3636363636364|    0|                 0.0|\n",
      "|90031-2009|              0.0|               27|        100.0|         41.0|     50.655849999999994|             215.0| 470.3703703703704|    1| 0.02557348541032657|\n",
      "|90211-2010|              0.0|              224|          0.0|       2917.0|      713.4047425750001|         3209.8355| 363.8482142857143|    1| 0.12470382840753212|\n",
      "|90505-2010|              0.0|              370|          0.0|       7000.0|     1012.2232050750001|         4442.3355| 394.9189189189189|    7|  0.1873260543780775|\n",
      "|90815-2010|              0.0|              199|          0.0|        441.0|             509.824625|            1990.0|488.94472361809045|    6| 0.14866572511707427|\n",
      "|90815-2011|              0.0|              234|          0.0|        597.0|      661.1660727249998|2529.6710000000003|466.79487179487177|    4| 0.09911048341138284|\n",
      "|91303-2007|              0.0|               97|          0.0|        191.0|     241.05254999999997|             887.5|425.25773195876286|    2| 0.06583278472679394|\n",
      "|91506-2009|              0.0|              137|          0.0|        216.0|     240.43338000000003|            1245.0|331.82481751824815|    3|  0.1644286105782406|\n",
      "|91602-2008|              0.0|               52|          0.0|        109.0|              119.18171|             585.0| 361.9230769230769|    1|0.052787162162162164|\n",
      "|91746-2006|              0.0|               10|         null|         14.0|                12.9987|              72.5|             340.0|    3|   0.097911227154047|\n",
      "+----------+-----------------+-----------------+-------------+-------------+-----------------------+------------------+------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "death_join_df = death_df.select('ZIP-YEAR','Count','SUI_per_thousand')\n",
    "drug_death_df = drug_agg_df.join(death_join_df, 'ZIP-YEAR', 'left_outer')\n",
    "drug_death_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 70.15643072128296 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
